train_zh_onto.sh: line 6: EXP-ID=22_1: command not found
Please notice that merge the args_dict and json_config ... ...
{
  "bert_frozen": "false",
  "hidden_size": 768,
  "hidden_dropout_prob": 0.2,
  "classifier_sign": "multi_nonlinear",
  "clip_grad": 1,
  "bert_config": {
    "attention_probs_dropout_prob": 0.1,
    "directionality": "bidi",
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "max_position_embeddings": 512,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "pooler_fc_size": 768,
    "pooler_num_attention_heads": 12,
    "pooler_num_fc_layers": 3,
    "pooler_size_per_head": 128,
    "pooler_type": "first_token_transform",
    "type_vocab_size": 2,
    "vocab_size": 21128
  },
  "config_path": "/data/xiaoya/work/mrc-for-flat-nested-ner/config/zh_bert.json",
  "data_dir": "/data/xiaoya/work/datasets/mrc_ner/zh_onto4",
  "bert_model": "/data/nfsdata/nlp/BERT_BASE_DIR/chinese_L-12_H-768_A-12",
  "task_name": null,
  "max_seq_length": 100,
  "train_batch_size": 12,
  "dev_batch_size": 32,
  "test_batch_size": 32,
  "checkpoint": 600,
  "learning_rate": 8e-06,
  "num_train_epochs": 6,
  "warmup_proportion": -1.0,
  "local_rank": -1,
  "gradient_accumulation_steps": 1,
  "seed": 2333,
  "export_model": true,
  "output_dir": "/data/xiaoya/output_mrc_ner/zh_onto/mrc-ner-zh_onto--100-8e-6-12-0.2",
  "data_sign": "zh_onto",
  "weight_start": 1.0,
  "weight_end": 1.0,
  "weight_span": 1.0,
  "entity_sign": "flat",
  "n_gpu": 1,
  "dropout": 0.2,
  "entity_threshold": 0.5,
  "data_cache": true
}
-*--*--*--*--*--*--*--*--*--*-
current data_sign: zh_onto
=*==*==*==*==*==*==*==*==*==*=
loading train data ... ...
62896
62896 train data loaded
=*==*==*==*==*==*==*==*==*==*=
loading dev data ... ...
17204
17204 dev data loaded
=*==*==*==*==*==*==*==*==*==*=
loading test data ... ...
17384
17384 test data loaded
######################################################################
EPOCH:  0
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.00030672221328131855
............................................................
DEV: loss, acc, precision, recall, f1
0.0169 0.7127 0.5697 0.4325 0.4917
SAVED model path is :
/data/xiaoya/output_mrc_ner/zh_onto/mrc-ner-zh_onto--100-8e-6-12-0.2/bert_finetune_model_0_600.bin
............................................................
TEST: loss, acc, precision, recall, f1
0.0175 0.7003 0.6056 0.4491 0.5157
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0010160376550629735
............................................................
DEV: loss, acc, precision, recall, f1
0.0119 0.8076 0.7508 0.6565 0.7005
SAVED model path is :
/data/xiaoya/output_mrc_ner/zh_onto/mrc-ner-zh_onto--100-8e-6-12-0.2/bert_finetune_model_0_1200.bin
............................................................
TEST: loss, acc, precision, recall, f1
0.0121 0.8001 0.7751 0.6597 0.7128
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0005721751949749887
............................................................
DEV: loss, acc, precision, recall, f1
0.0153 0.7491 0.5919 0.4912 0.5369
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0026511135511100292
............................................................
DEV: loss, acc, precision, recall, f1
0.0126 0.7989 0.7529 0.6113 0.6748
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.005567628890275955
............................................................
DEV: loss, acc, precision, recall, f1
0.0093 0.8611 0.8132 0.7622 0.7869
SAVED model path is :
/data/xiaoya/output_mrc_ner/zh_onto/mrc-ner-zh_onto--100-8e-6-12-0.2/bert_finetune_model_0_3000.bin
............................................................
TEST: loss, acc, precision, recall, f1
0.0094 0.8556 0.8283 0.7694 0.7978
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.003463444532826543
............................................................
DEV: loss, acc, precision, recall, f1
0.0092 0.861 0.8107 0.7589 0.7839
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.00012990817776881158
............................................................
DEV: loss, acc, precision, recall, f1
0.0097 0.8599 0.8668 0.7057 0.778
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.003122376510873437
............................................................
DEV: loss, acc, precision, recall, f1
0.0131 0.7846 0.8441 0.4174 0.5586
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  1
current learning rate 7.599999999999999e-06
current learning rate 7.599999999999999e-06
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
7.691606879234314e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0163 0.7595 0.6151 0.5526 0.5822
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
5.2304400014691055e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0111 0.8625 0.8166 0.7521 0.783
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0002458363014739007
............................................................
DEV: loss, acc, precision, recall, f1
0.015 0.7713 0.6688 0.5402 0.5977
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0019784190226346254
............................................................
DEV: loss, acc, precision, recall, f1
0.0115 0.8246 0.7643 0.6912 0.7259
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.00196545640937984
............................................................
DEV: loss, acc, precision, recall, f1
0.01 0.8642 0.8345 0.7424 0.7857
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0007872022688388824
............................................................
DEV: loss, acc, precision, recall, f1
0.01 0.8617 0.7983 0.7557 0.7765
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
5.866173160029575e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.011 0.8575 0.8431 0.7212 0.7774
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0017828171839937568
............................................................
DEV: loss, acc, precision, recall, f1
0.0148 0.7838 0.7614 0.4689 0.5804
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  2
current learning rate 7.219999999999999e-06
current learning rate 7.219999999999999e-06
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
6.5534288296476e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0174 0.7611 0.6043 0.5476 0.5745
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
4.9239417421631515e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0115 0.8657 0.8119 0.7689 0.7898
SAVED model path is :
/data/xiaoya/output_mrc_ner/zh_onto/mrc-ner-zh_onto--100-8e-6-12-0.2/bert_finetune_model_2_1200.bin
............................................................
TEST: loss, acc, precision, recall, f1
0.0111 0.8623 0.8171 0.7862 0.8014
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
6.625332025578246e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.016 0.7869 0.7013 0.5538 0.6189
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.000157595903147012
............................................................
DEV: loss, acc, precision, recall, f1
0.0117 0.8359 0.7953 0.7177 0.7545
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0002080426347674802
............................................................
DEV: loss, acc, precision, recall, f1
0.0108 0.8768 0.8193 0.7978 0.8084
SAVED model path is :
/data/xiaoya/output_mrc_ner/zh_onto/mrc-ner-zh_onto--100-8e-6-12-0.2/bert_finetune_model_2_3000.bin
............................................................
TEST: loss, acc, precision, recall, f1
0.0105 0.8798 0.8303 0.8135 0.8218
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.000440346950199455
............................................................
DEV: loss, acc, precision, recall, f1
0.011 0.8615 0.8039 0.7691 0.7861
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
4.2199812014587224e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0119 0.8525 0.8387 0.7185 0.774
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0018308220896869898
............................................................
DEV: loss, acc, precision, recall, f1
0.0146 0.8053 0.8055 0.5476 0.652
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  3
current learning rate 6.858999999999998e-06
current learning rate 6.858999999999998e-06
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
4.799824091605842e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0156 0.7964 0.6551 0.6538 0.6545
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
3.305499558337033e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0118 0.8593 0.8167 0.7473 0.7805
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
7.416812877636403e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0168 0.798 0.7087 0.5576 0.6241
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0002161661977879703
............................................................
DEV: loss, acc, precision, recall, f1
0.0126 0.8345 0.7974 0.6912 0.7405
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0003091798862442374
............................................................
DEV: loss, acc, precision, recall, f1
0.0116 0.8757 0.8188 0.7823 0.8001
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0003750305622816086
............................................................
DEV: loss, acc, precision, recall, f1
0.0125 0.8577 0.8128 0.7411 0.7753
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
2.8040232791681774e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0134 0.8615 0.8218 0.748 0.7832
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0021460324060171843
............................................................
DEV: loss, acc, precision, recall, f1
0.0157 0.8066 0.8186 0.5675 0.6703
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  4
current learning rate 6.516049999999998e-06
current learning rate 6.516049999999998e-06
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
2.615754601720255e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.015 0.8201 0.7125 0.6815 0.6966
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
2.4680895876372233e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0124 0.8548 0.8069 0.7386 0.7712
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
3.375844971742481e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0176 0.8028 0.7304 0.5783 0.6455
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
7.256292155943811e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.014 0.8271 0.7935 0.6551 0.7177
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0006521830218844116
............................................................
DEV: loss, acc, precision, recall, f1
0.0122 0.8754 0.799 0.8055 0.8022
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0002802903763949871
............................................................
DEV: loss, acc, precision, recall, f1
0.0132 0.8574 0.8267 0.74 0.781
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
3.069755985052325e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0139 0.8595 0.8262 0.7457 0.7839
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0002819267101585865
............................................................
DEV: loss, acc, precision, recall, f1
0.0149 0.8289 0.8361 0.6477 0.7299
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
######################################################################
EPOCH:  5
current learning rate 6.190247499999998e-06
current learning rate 6.190247499999998e-06
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
2.8775666578439996e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0133 0.8531 0.7692 0.7444 0.7566
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
2.9680761144845746e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0127 0.8583 0.793 0.7535 0.7728
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
4.135211929678917e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0164 0.8192 0.7732 0.6116 0.683
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
9.956182111636735e-06
............................................................
DEV: loss, acc, precision, recall, f1
0.0143 0.8297 0.7937 0.6537 0.7169
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0005675724241882563
............................................................
DEV: loss, acc, precision, recall, f1
0.0123 0.8678 0.7883 0.8013 0.7947
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.0003134422586299479
............................................................
DEV: loss, acc, precision, recall, f1
0.0131 0.862 0.8261 0.7545 0.7886
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
0.00010929797281278297
............................................................
DEV: loss, acc, precision, recall, f1
0.0134 0.861 0.8264 0.7527 0.7879
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
current training loss is :
9.037394193001091e-05
............................................................
DEV: loss, acc, precision, recall, f1
0.0133 0.8614 0.8248 0.7545 0.7881
-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-
=&==&==&==&==&==&==&==&==&==&==&==&==&==&==&=
Best DEV : overall best loss, acc, precision, recall, f1
0.0108 0.8768 0.8193 0.7978 0.8084
scores on TEST when Best DEV:loss, acc, precision, recall, f1
0.0105 0.8798 0.8303 0.8135 0.8218
=&==&==&==&==&==&==&==&==&==&==&==&==&==&==&=